{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf25592-ca00-4759-ad67-63bdfeee3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.google_search as gs\n",
    "import ipynb.fs.full.gather_hl_and_articles as reader\n",
    "import ipynb.fs.full.open_ai_models as openaimodel\n",
    "import ipynb.fs.full.data_operations as do\n",
    "import pandas as pd\n",
    "import pprint as pp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406a589e-0398-41d3-a518-4de64af99199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    analysts = ['Sumeet Bagadia'#,\n",
    "    #        'Vaishali Parekh', \n",
    "    #        'Ganesh Dongre',\n",
    "    #        'Dharmesh Shah',\n",
    "    #        'Seema Srivastava',\n",
    "    #        'Anshul Jain',\n",
    "    #        'Shiju Koothupalakkal', \n",
    "    #        'Sugandha Sachdeva',\n",
    "    #        'Mahesh M Ojha'\n",
    "                ]\n",
    "    \n",
    "    \n",
    "    ops = do.gspread_Operator()\n",
    "    srch = gs.google_Search()\n",
    "    llm = openaimodel.OpenAIModels()\n",
    "    rdr = reader.Livemint_reader()\n",
    "    \n",
    "    #for loop should begin here:\n",
    "    for analyst in analysts:\n",
    "        try:\n",
    "            print(f'Starting mass link search for {analyst}...')\n",
    "            link_ids_already_seen = ops.read_specific_article_columns(['link_id'])\n",
    "            #print(link_ids_already_seen.shape)\n",
    "            google_links = srch.search(query=analyst, terms='Buy', num_pages=10, silent_mode=False)\n",
    "            print(f'Found {google_links.shape[0]} links for {analyst} from Google for Livemint. Proceeding to dedupe against known links...')\n",
    "            ##remove links we have previously seen\n",
    "            seen_indices = google_links[google_links['link_id'].isin(link_ids_already_seen['link_id'])].index.tolist()\n",
    "            if(seen_indices):\n",
    "                google_links.drop(index=seen_indices, inplace=True)\n",
    "            print(f'Now remaining {google_links.shape[0]} links after dedupe...')\n",
    "            response = llm.classify_headlines(google_links['title'], silent_mode=False)\n",
    "            if response:\n",
    "                output_json = json.loads(response)['output']\n",
    "                classified_hl = pd.DataFrame(output_json)\n",
    "                classified_titles = google_links.join(classified_hl, lsuffix='_orig', rsuffix='_copy')\n",
    "                #ops.write_headlines_in_chunks(classified_titles)\n",
    "                for idx, row in classified_titles:\n",
    "                    article = rdr.read_article(row['link'])\n",
    "                    if(article):\n",
    "                        response2 = llm.gather_articles_susbtance(article, silent_mode=False)\n",
    "                        if response2:\n",
    "                            recos = pd.DataFrame(json.loads(response2)['output'])\n",
    "                            ops.write_recommendations(recos)\n",
    "                            ops.write_headlines(row)\n",
    "                            break\n",
    "                        else:\n",
    "                            print(f'No recommendation could be gathered for {row['link']}...')\n",
    "                    else:\n",
    "                        print(f'No article could be retrieved for {row['link']}...') \n",
    "                    break\n",
    "            else:\n",
    "                print(f'No classifications could be done for the articles...')\n",
    "        except Exception as e:\n",
    "            print(f'Failed execution with exception: {e}')\n",
    "        finally:\n",
    "            continue\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b47a91f-b21b-4afc-b5bd-90de23cb3b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting mass link search for Sumeet Bagadia...\n",
      "Running search for Sumeet+Bagadia on page: 1\n",
      "Error: 429 at run page: 1\n",
      "Combining all outcomes...\n",
      "Deduping links...\n",
      "Found 0 links for Sumeet Bagadia from Google for Livemint. Proceeding to dedupe against known links...\n",
      "Now remaining 0 links after dedupe...\n",
      "Sending titles for classification to OpenAI model...\n",
      "Failed execution with exception: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    hl = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedfe81-1242-4e75-8072-1e5fb77951ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
