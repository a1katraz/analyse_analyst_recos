{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6865b643-1387-446c-8adc-5cd4ed82ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import base64\n",
    "import os\n",
    "import bs4\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from bs4 import BeautifulSoup \n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from openpyxl import load_workbook\n",
    "import uuid\n",
    "import random\n",
    "import pprint\n",
    "import dotenv as env\n",
    "env.load_dotenv('../keys.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0166563-a7e7-44e0-8532-8a938fed87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_livemint_headlines():\n",
    "    url = \"https://www.livemint.com/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Get the top headlines in the first ATF block\n",
    "\n",
    "    # always better to grow arrays first then form a dataframe from them\n",
    "    rel_titles = []\n",
    "    rel_links = []\n",
    "    rel_link_id = []\n",
    "    uuids = []\n",
    "\n",
    "    if(response.status_code == requests.codes.ok):\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hero_stories = soup.select('.heroStory .imgStory a')\n",
    "        if(hero_stories):\n",
    "            for story in hero_stories:\n",
    "                rel_titles.append(story.contents[0].strip())\n",
    "                rel_links.append(story.get('href'))\n",
    "                rel_link_id.append( story.get('href')[-19:][:14])\n",
    "                uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                uuids.append(uuid_num)\n",
    "                #print(story.get('href'))\n",
    "                #print(story.contents[0].strip())\n",
    "                \n",
    "        news_stories = soup.select('li.newsBlock h3 a')\n",
    "        if(news_stories):\n",
    "            for story in news_stories:\n",
    "                rel_titles.append(story.contents[0].strip())\n",
    "                rel_links.append(story.get('href'))\n",
    "                rel_link_id.append( story.get('href')[-19:][:14])\n",
    "                uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                uuids.append(uuid_num)\n",
    "                #print(story.get('href'))\n",
    "                #print(story.contents[0].strip())\n",
    "        \n",
    "        if(hero_stories or news_stories):\n",
    "            top_headlines = pandas.DataFrame({\n",
    "                                                    'uuid': uuids, \n",
    "                                                    'title': rel_titles,     #Don't change these column names, they are important for merging later \n",
    "                                                    'link': rel_links,\n",
    "                                                    'link_id': rel_link_id\n",
    "                                                })\n",
    "            top_headlines.insert(1, 'site', 'Livemint')\n",
    "            return top_headlines\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6612c114-b9be-4460-906c-499af1247ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_et_headlines():\n",
    "    #'''This function can produce duplicates in its returned dataframe. This is because ET website is not structured properly. Eliminate the duplicates using the rel_link_id in postprocessig'''\n",
    "    url = 'https://economictimes.indiatimes.com/'\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # always better to grow arrays first then form a dataframe from them\n",
    "    rel_titles = []\n",
    "    rel_links = []\n",
    "    rel_link_id = []\n",
    "    uuids = []\n",
    "\n",
    "    if(response.status_code == requests.codes.ok):\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hero_stories = soup.select('#investIdeas ul.newsList li a')\n",
    "        if(hero_stories):\n",
    "            for story in hero_stories:\n",
    "                rel_titles.append(story.get_text())                   #only 1 span inside the headline which contains the link texct which we read\n",
    "                link = story.get('href').split('?')[0]                # source=homepage&medium=investment_ideas_content&campaign=prime_discovery is junk and can be removed\n",
    "                rel_links.append(f'https://economictimes.indiatimes.com{link}')\n",
    "                rel_link_id.append(link[-13:][:9])\n",
    "                uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                uuids.append(uuid_num)\n",
    "                #print(story.get('href'))\n",
    "                #print(story.select('span'))\n",
    "        else:\n",
    "            print('No hero stories found')\n",
    "        \n",
    "        news_stories = soup.select('ul.newsList li a')\n",
    "        if(news_stories):\n",
    "            for story in news_stories:\n",
    "                text = story.get_text()\n",
    "                link = story.get('href').split('?')[0]                                     # source=homepage&medium=investment_ideas_content&campaign=prime_discovery is junk and can be removed\n",
    "                if(text!='ET MARKETS' and text!='' and 'timeslearn'not in link):                             # No need to process these links\n",
    "                    rel_titles.append(text)\n",
    "                    if('https://economictimes.indiatimes.com' not in link):                       # ET links are not consistent in design\n",
    "                        rel_links.append(f'https://economictimes.indiatimes.com{link}')\n",
    "                    else:\n",
    "                        rel_links.append(link)\n",
    "                    rel_link_id.append(link[-13:][:9])\n",
    "                    uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                    uuids.append(uuid_num)\n",
    "                    #print(story.get('href'))\n",
    "                    #print(story.contents[0].strip())\n",
    "        else:\n",
    "            print('No topline stories found')\n",
    "        if(hero_stories or news_stories):\n",
    "            top_headlines = pandas.DataFrame({\n",
    "                                                    'uuid': uuids, \n",
    "                                                    'title': rel_titles,     #Don't change these column names, they are important for merging later \n",
    "                                                    'link': rel_links,\n",
    "                                                    'link_id': rel_link_id\n",
    "                                                })\n",
    "            top_headlines.insert(1, 'site', 'ET')\n",
    "            return top_headlines\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b54dedd6-c9ec-478f-bb15-854eb77d4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_livemint_extra():\n",
    "    # Removing this function because the upper function is simpler and more comprehensive.\n",
    "    url = \"https://www.livemint.com/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    # Get the top headlines in the first ATF block\n",
    "\n",
    "    # alwasy better to grow arrays first then form a dataframe from them\n",
    "    rel_titles = []\n",
    "    rel_links = []\n",
    "    uuids = []\n",
    "    \n",
    "    #parsing the page now for relevant links and story titles\n",
    "    if(response.status_code == requests.codes.ok):\n",
    "        #print('Access good')\n",
    "        #print(response.status_code)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        central_block = soup.find(\"div\", {\"class\": \"designCol3\"})\n",
    "        if(central_block):\n",
    "            blocks = central_block.find_all('div', {'class': 'contentBox'})\n",
    "            if(blocks):\n",
    "                for block in blocks[:2]:\n",
    "                    hero_link = block.find('li', {'class': 'heroStory'}) #only one herostory\n",
    "                    if(hero_link):   #heroplink is only in block 1 and not block 2\n",
    "                        main_text = hero_link.select('h2 a')  #bs4.element.Tag\n",
    "                        li_name = main_text.pop(0) \n",
    "                        rel_titles.append(li_name.contents[0])\n",
    "                        rel_links.append(li_name.get('href'))\n",
    "                        \n",
    "                        uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                        uuids.append(uuid_num)\n",
    "                        \n",
    "                        #print(li_name.get('href'))\n",
    "                        #print(li_name.contents[0])\n",
    "                    block_links = block.find_all('li', {'class': 'newsBlock'})  #we leave aside the herostory block\n",
    "                    for link in block_links:\n",
    "                        #print(link) #link is bs4.element.ResultSet\n",
    "                        main_text = link.select('h3 a')  #bs4.element.Tag\n",
    "                        li_name = main_text.pop(0) \n",
    "                        rel_titles.append(li_name.contents[0])\n",
    "                        rel_links.append(li_name.get('href'))\n",
    "                        \n",
    "                        uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                        uuids.append(uuid_num)\n",
    "                        #print(li_name.get('href'))\n",
    "                        #print(li_name.contents[0])                    \n",
    "                        #print(uuid_num)                \n",
    "                #top_headlines.insert(0, 'date', datetime.today().strftime('%Y-%m-%d'))  #removed in favor of date from the article itself.\n",
    "                #return top_headlines\n",
    "                #print (top_headlines)\n",
    "            else:\n",
    "                print('Vertical main content blocks not found')\n",
    "                #return None\n",
    "        else:\n",
    "            print('Main content block not found')\n",
    "            #return None\n",
    "\n",
    "        # This block does not work because the rightBlockNew because this block is not \n",
    "        # .storyList element exists in the DOM, but it's not a direct child of body#search. \n",
    "        # The presence of div#__next as a child of the body suggests this might be a single-page application (SPA), likely built with React. \n",
    "        # In SPAs, content is often loaded dynamically after the initial page load. This means that the .storyList element is likely rendered by JavaScript after your initial code has run. That is why you were not able to find the element.\n",
    "        # This can only be resolved by a Selenium execution\n",
    "        \n",
    "        right_block = soup.find('div', {'class': 'rightBlockNew'})\n",
    "        print(right_block.prettify())\n",
    "        if(right_block):\n",
    "            link_divs = right_block.find_all('ul', {'class': 'newsLinsting'})\n",
    "            #print(link_divs.prettify())\n",
    "            for div in link_divs:\n",
    "                readable_li = div.find('div', {'class': 'storyList'})\n",
    "                print(div.prettify())\n",
    "                main_text = readable_li.select('a')  #bs4.element.Tag\n",
    "                li_name = main_text.pop(0) \n",
    "                rel_titles.append(li_name.contents[0])\n",
    "                rel_links.append(li_name.get('href'))\n",
    "\n",
    "                print(li_name.get('href'))\n",
    "                print(li_name.contents[0]) \n",
    "                \n",
    "                uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                uuids.append(uuid_num)\n",
    "                \n",
    "        else:\n",
    "            print('Right content block not found')\n",
    "        \n",
    "        if((central_block)or (right_block)):\n",
    "            top_headlines = pandas.DataFrame({\n",
    "                                                    'uuid': uuids, \n",
    "                                                    'title': rel_titles,     #Don't change these column names, they are important for merging later \n",
    "                                                    'link': rel_links\n",
    "                                                })\n",
    "            top_headlines.insert(1, 'site', 'Livemint')\n",
    "            return top_headlines\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        print('Access to Livemint is failing')\n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a0ec3c-6178-4e00-a64f-e5c805e00dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't change these names, they are imporrtant for data processing later \n",
    "\n",
    "class Classifications(BaseModel):\n",
    "    title: str\n",
    "    classification: bool\n",
    "    explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec214e4-026c-4214-9cc1-ebdb302f7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ther main headlines to see which ones have stock recommendations\n",
    "# Produces output in a schema set above in class Classifications as a JSON string\n",
    "\n",
    "def classify_headlines(headlines):\n",
    "    client = genai.Client(\n",
    "            #api_key = GEMINI_API_KEY\n",
    "            api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        )\n",
    "    \n",
    "    model = 'gemini-2.0-flash-lite'\n",
    "    #gemini-2.0-flash-lite'\n",
    "    #\"gemini-2.5-pro-exp-03-25\"\n",
    "    # see list here: https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-03-25\n",
    "\n",
    "    print(f'Executing headline classification for {headlines.size} headlines')\n",
    "    \n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "                                temperature=0.8,\n",
    "                                #response_mime_type=\"text/plain\", # for simpler use cases where only text is expected from the model \n",
    "                                system_instruction=[\n",
    "                                                    types.Part.from_text(text=\"\"\"You are a text analyser to classify if a given sentence has a recommendation for acting on any stocks. \n",
    "                                                    You are given a json list of individual sentences. Operate on each sentence as per the following logic.\n",
    "                                                    Looking only at the contents of each the sentences and identifying words like *recommend*, *buy*, and phrases like *what to buy*, *which to buy*, *buy or sell*, you classify \n",
    "                                                    if the sentence points to an explicit recommendation.\n",
    "                                                    Your output should be a True or a False where True means that the sentence has a stock recommendation and False means it does not.\n",
    "                                                    Explain your classifying logic for the given sentence in 50 characters.\n",
    "                                                    Return your response of the original sentence, your classification and your logic against each sentence.\n",
    "                                                    \"\"\"),\n",
    "                                            ],\n",
    "                                response_mime_type='application/json',\n",
    "                                response_schema=list[Classifications],    #force response in a structured format from Gemini\n",
    "                                )\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(text=headlines.to_json()),  #this part has to be fixed to send a json list of strings to the model. Otherwise cheaper models can mix up the sentence and the delimiter and miss some sentences - is Done.  \n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    \n",
    "    model_reply = client.models.generate_content(model=model, contents=contents, config=generate_content_config)\n",
    "    #print((model_reply))\n",
    "    #for chunk in client.models.generate_content_stream(model=model, contents=contents, config=generate_content_config):\n",
    "    #    model_reply = model_reply + chunk.text\n",
    "    return model_reply.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e97e346-a08a-4e3d-9154-63286678d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_livemint2():\n",
    "    url = \"https://www.livemint.com/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    #list_items = soup.find_all(style=lambda value: value and 'display:none' in value.replace(' ', '').lower())\n",
    "    #story_lists = soup.select('.heroStory .imgStory a')\n",
    "    #for story in story_lists:\n",
    "    #    print(story.get('href'))\n",
    "    #    print(story.contents[0].strip())\n",
    "    \n",
    "    story_lists = soup.select('li.newsBlock h3 a')\n",
    "    for story in story_lists:\n",
    "        print(story.get('href'))\n",
    "        print(story.contents[0].strip())\n",
    "        \n",
    "    #print(story.get('href'))\n",
    "    #print(story.contents[0].strip())\n",
    "        \n",
    "    #right_block = soup.find('div', {'class': 'rightBlockNew'})\n",
    "    #print(right_block.prettify())\n",
    "    #for item in list_items:\n",
    "    #    print(item.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b474ac5-4b9f-45d5-bb34-a67e6c803221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one line tester\n",
    "#hl = read_et_headlines()\n",
    "#hl.drop_duplicates(subset=['link_id'], keep='first', inplace=True, ignore_index=True)\n",
    "#hl.to_csv('et_hl_text.csv')\n",
    "#print(hl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "739385f3-8027-4d35-8662-f634f249a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One line tester.\n",
    "\n",
    "#outcome = classify_headlines(pandas.Series(['BJPs jibe at Cong over National Herald case: Not Indira Gandhis emergency',\n",
    "#                                            'Buy or sell: Sumeet Bagadia recommends three stocks to buy on Monday â€” 21 April'\n",
    "#                                           ]))   #no longer valid as we are now sending a Series\n",
    "#outdf = pandas.DataFrame(json.loads(outcome))\n",
    "#outdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2867235-1644-4e7a-8208-08aa9526a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
