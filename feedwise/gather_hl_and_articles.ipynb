{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4e788e-8a53-41e9-80b9-9fab5f664aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup \n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52c9b22-8cd1-48ee-9c1b-f7ce0aa038f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Livemint_reader:\n",
    "    #__base_url: str\n",
    "    __headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119 Safari/537.36'}\n",
    "    \n",
    "    def __init__(self, url=''):\n",
    "        if(url):\n",
    "            self.__base_url = url\n",
    "        else:\n",
    "            self.__base_url = 'https://www.livemint.com/'\n",
    "        \n",
    "    \n",
    "    def find_articles_on_home_page(self) -> pandas.DataFrame:         \n",
    "        ''' Made redundant with feed reading\n",
    "        '''\n",
    "        response = requests.get(url=self.__base_url, headers=Livemint_reader.__headers)\n",
    "        # Get the top headlines in the first ATF block\n",
    "        # always better to grow arrays first then form a dataframe from them\n",
    "        rel_titles = []\n",
    "        rel_links = []\n",
    "        rel_link_id = []\n",
    "        uuids = []\n",
    "\n",
    "        if(response.status_code == requests.codes.ok):\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            hero_stories = soup.select('.heroStory .imgStory a')\n",
    "            if(hero_stories):\n",
    "                for story in hero_stories:\n",
    "                    rel_titles.append(story.contents[0].strip())\n",
    "                    rel_links.append(story.get('href'))\n",
    "                    rel_link_id.append( story.get('href')[-19:][:14])\n",
    "                    uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                    uuids.append(uuid_num)\n",
    "                    #print(story.get('href'))\n",
    "                    #print(story.contents[0].strip())\n",
    "                \n",
    "            news_stories = soup.select('li.newsBlock h3 a')\n",
    "            if(news_stories):\n",
    "                for story in news_stories:\n",
    "                    rel_titles.append(story.contents[0].strip())\n",
    "                    rel_links.append(story.get('href'))\n",
    "                    rel_link_id.append( story.get('href')[-19:][:14])\n",
    "                    uuid_num = uuid.UUID(int=random.getrandbits(128), version=4)\n",
    "                    uuids.append(uuid_num)\n",
    "                    #print(story.get('href'))\n",
    "                    #print(story.contents[0].strip())\n",
    "            \n",
    "            if(hero_stories or news_stories):\n",
    "                top_headlines = pandas.DataFrame({\n",
    "                                                    'uuid': uuids, \n",
    "                                                    'title': rel_titles,     #Don't change these column names, they are important for merging later \n",
    "                                                    'link': rel_links,\n",
    "                                                    'link_id': rel_link_id\n",
    "                                                })\n",
    "                top_headlines.insert(1, 'site', 'Livemint')\n",
    "                return top_headlines\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def read_article(self, art_url:str) -> pandas.DataFrame:\n",
    "        headline = ''\n",
    "        subhead = ''\n",
    "        body = ''\n",
    "        art_date = ''\n",
    "        \n",
    "        self.__base_url = art_url  #This is bad design\n",
    "        \n",
    "        #art_url = top_headlines.iat[2, 1]\n",
    "        art_res = requests.get(url=self.__base_url, headers=Livemint_reader.__headers)\n",
    "        if(art_res.status_code == requests.codes.ok):\n",
    "            #Done\n",
    "            art_soup = BeautifulSoup(art_res.content, 'html.parser')\n",
    "            head_h1 = art_soup.find('h1', {'id':'article-0'})  #This might not be affected by changes in web page \n",
    "            if head_h1:\n",
    "                headline = head_h1.get_text()\n",
    "            else:\n",
    "                print(f'{art_url}: No headline found')\n",
    "\n",
    "            subhead_pattern = re.compile(r'storyPage_summary.*')\n",
    "            sub_head_h2 = art_soup.find('h2', class_=subhead_pattern) #This might fail on a daily basis, just switch to using h2 then \n",
    "            if sub_head_h2:\n",
    "                subhead = sub_head_h2.get_text() # Can be used to improve sharpness of LLM \n",
    "            else:\n",
    "                print(f'{art_url}: No subhead found')\n",
    "            \n",
    "            art_body_divs = art_soup.find_all('div', {'class': 'storyParagraph'}) # should work well and not fail\n",
    "            if art_body_divs:\n",
    "                for para_div in art_body_divs:\n",
    "                    para = para_div.get_text()\n",
    "                    if 'Disclaimer' not in para:   #reject boiler plate text to reduce tokens to LLMs\n",
    "                        if(body):\n",
    "                            body = body + '\\n' +  para\n",
    "                        else:\n",
    "                            body = para\n",
    "            else:\n",
    "                print(f'{art_url}: No Article Body found')\n",
    "\n",
    "            date_pattern = re.compile(r'storyPage_date.*')\n",
    "            art_date_div = art_soup.find('div', class_=date_pattern) #find date\n",
    "            if art_date_div:\n",
    "                art_date = art_date_div.get_text()\n",
    "            else:\n",
    "                print(f'{art_url}: No article date found')\n",
    "            #print (body)\n",
    "            article_frame = pandas.DataFrame([[headline, subhead, body, art_date]], columns=['article_title', 'article_subheader', 'article_body', 'article_date'])\n",
    "            return article_frame\n",
    "        else:\n",
    "            print('Access to specific article' + art_url + 'is failing')\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5287d6f6-a0d6-4f73-a9f1-95e90bcf43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tester\n",
    "#rdr = Livemint_reader()\n",
    "#home_links = rdr.find_articles_on_home_page()\n",
    "#pp.pprint(home_links)\n",
    "#art1 = rdr.read_article(home_links.iloc[0, 3])\n",
    "#pp.pprint(art1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053e3b6-2b9e-40a3-81bd-d9f50390e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
